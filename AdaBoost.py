# scikit-learnä¸­Adaboostç±»åº“æ¯”è¾ƒç›´æ¥ï¼Œå°±æ˜¯AdaBoostClassifierå’ŒAdaBoostRegressorä¸¤ä¸ª
# ä»åå­—å°±èƒ½çœ‹å‡ºå®ƒä»¬äºŒè€…çš„åŒºåˆ«

# AdaBoostClassifierä½¿ç”¨äº†ä¸¤ç§Adabooståˆ†ç±»ç®—æ³•çš„å®ç°ï¼ŒSAMMEå’ŒSAMME.R
# AdaBoostRegressoråˆ™ä½¿ç”¨äº†Adaboostå›å½’ç®—æ³•çš„å®ç°ï¼Œå³Adaboost.R2

# å½“æˆ‘ä»¬å¯¹Adaboostè°ƒå‚æ—¶ï¼Œä¸»è¦è¦å¯¹ä¸¤éƒ¨åˆ†å†…å®¹è¿›è¡Œè°ƒå‚
# ç¬¬ä¸€éƒ¨åˆ†æ˜¯å¯¹æˆ‘ä»¬çš„Adaboostçš„æ¡†æ¶è¿›è¡Œè°ƒå‚
# ç¬¬äºŒéƒ¨åˆ†æ˜¯å¯¹æˆ‘ä»¬é€‰æ‹©çš„å¼±åˆ†ç±»å™¨è¿›è¡Œè°ƒå‚
# ä¸¤è€…ç›¸è¾…ç›¸æˆ

# æˆ‘ä»¬é¦–å…ˆæ¥çœ‹AdaBoostClassifierå’ŒAdaBoostRegressoræ¡†æ¶å‚æ•°ï¼Œé‡è¦å‚æ•°å¦‚ä¸‹ï¼š
# 1. base_estimator
# AdaBoostClassifierå’ŒAdaBoostRegressoréƒ½æœ‰ï¼Œå³æˆ‘ä»¬çš„å¼±åˆ†ç±»å­¦ä¹ å™¨æˆ–è€…å¼±å›å½’å­¦ä¹ å™¨
# ç†è®ºä¸Šå¯ä»¥é€‰æ‹©ä»»ä½•ä¸€ä¸ªåˆ†ç±»æˆ–è€…å›å½’å­¦ä¹ å™¨ï¼Œä¸è¿‡éœ€è¦æ”¯æŒæ ·æœ¬æƒé‡,æˆ‘ä»¬å¸¸ç”¨çš„ä¸€èˆ¬æ˜¯CARTå†³ç­–æ ‘æˆ–è€…ç¥ç»ç½‘ç»œMLPã€‚é»˜è®¤æ˜¯å†³ç­–æ ‘

# 2. algorithm
# è¿™ä¸ªå‚æ•°åªæœ‰AdaBoostClassifieræœ‰ã€‚ä¸»è¦åŸå› æ˜¯scikit-learnå®ç°äº†ä¸¤ç§Adabooståˆ†ç±»ç®—æ³•ï¼ŒSAMMEå’ŒSAMME.R
# ä¸¤è€…çš„ä¸»è¦åŒºåˆ«æ˜¯å¼±å­¦ä¹ å™¨æƒé‡çš„åº¦é‡
# SAMMEä½¿ç”¨äº†äºŒå…ƒåˆ†ç±»Adaboostç®—æ³•çš„æ‰©å±•ï¼Œå³ç”¨å¯¹æ ·æœ¬é›†åˆ†ç±»æ•ˆæœä½œä¸ºå¼±å­¦ä¹ å™¨æƒé‡
# SAMME.Rä½¿ç”¨äº†å¯¹æ ·æœ¬é›†åˆ†ç±»çš„é¢„æµ‹æ¦‚ç‡å¤§å°æ¥ä½œä¸ºå¼±å­¦ä¹ å™¨æƒé‡
# ç”±äºSAMME.Rä½¿ç”¨äº†æ¦‚ç‡åº¦é‡çš„è¿ç»­å€¼ï¼Œè¿­ä»£ä¸€èˆ¬æ¯”SAMMEå¿«ï¼Œå› æ­¤AdaBoostClassifierçš„é»˜è®¤ç®—æ³•algorithmçš„å€¼ä¹Ÿæ˜¯SAMME.Rï¼Œæˆ‘ä»¬ä¸€èˆ¬ä½¿ç”¨é»˜è®¤çš„SAMME.Rå°±å¤Ÿäº†
# ä½†æ˜¯è¦æ³¨æ„çš„æ˜¯ä½¿ç”¨äº†SAMME.Rï¼Œ åˆ™å¼±åˆ†ç±»å­¦ä¹ å™¨å‚æ•°base_estimatorå¿…é¡»é™åˆ¶ä½¿ç”¨æ”¯æŒæ¦‚ç‡é¢„æµ‹çš„åˆ†ç±»å™¨ã€‚SAMMEç®—æ³•åˆ™æ²¡æœ‰è¿™ä¸ªé™åˆ¶

# 3. loss
# è¿™ä¸ªå‚æ•°åªæœ‰AdaBoostRegressoræœ‰ï¼ŒAdaboost.R2ç®—æ³•éœ€è¦ç”¨åˆ°
# æœ‰çº¿æ€§â€˜linearâ€™, å¹³æ–¹â€˜squareâ€™å’ŒæŒ‡æ•° â€˜exponentialâ€™ä¸‰ç§é€‰æ‹©, é»˜è®¤æ˜¯çº¿æ€§ï¼Œä¸€èˆ¬ä½¿ç”¨çº¿æ€§å°±è¶³å¤Ÿäº†ï¼Œé™¤éä½ æ€€ç–‘è¿™ä¸ªå‚æ•°å¯¼è‡´æ‹Ÿåˆç¨‹åº¦ä¸å¥½

# 4. n_estimator
# AdaBoostClassifierå’ŒAdaBoostRegressoréƒ½æœ‰ï¼Œå°±æ˜¯æˆ‘ä»¬çš„å¼±å­¦ä¹ å™¨çš„æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œæˆ–è€…è¯´æœ€å¤§çš„å¼±å­¦ä¹ å™¨çš„ä¸ªæ•°
# è¿‡å¤šä¼šè¿‡æ‹Ÿåˆï¼Œè¿‡å°‘ä¼šæ¬ æ‹Ÿåˆï¼Œé»˜è®¤æ˜¯50
# å®é™…è°ƒå‚è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¸¸å¸¸å°†n_estimatorå’Œlearning_rateä¸€èµ·è€ƒè™‘

# 5. learning_rate
#  AdaBoostClassifierå’ŒAdaBoostRegressoréƒ½æœ‰ï¼Œå³æ¯ä¸ªå¼±å­¦ä¹ å™¨çš„æƒé‡ç¼©å‡ç³»æ•°ğœˆ
# è¾ƒå°çš„ğœˆæ„å‘³ç€æˆ‘ä»¬éœ€è¦æ›´å¤šçš„å¼±å­¦ä¹ å™¨çš„è¿­ä»£æ¬¡æ•°
# ä¸€èˆ¬æ¥è¯´ï¼Œå¯ä»¥ä»ä¸€ä¸ªå°ä¸€ç‚¹çš„ğœˆå¼€å§‹è°ƒå‚ï¼Œé»˜è®¤æ˜¯1

# å¼±å­¦ä¹ å™¨å‚æ•°ï¼Œå‚è€ƒCARTå†³ç­–æ ‘çš„å‚æ•°è®¾å®š

import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_gaussian_quantiles

# ç”Ÿæˆ2ç»´æ­£æ€åˆ†å¸ƒï¼Œç”Ÿæˆçš„æ•°æ®æŒ‰åˆ†ä½æ•°åˆ†ä¸ºä¸¤ç±»ï¼Œ500ä¸ªæ ·æœ¬,2ä¸ªæ ·æœ¬ç‰¹å¾ï¼Œåæ–¹å·®ç³»æ•°ä¸º2
X1, y1 = make_gaussian_quantiles(cov=2.0, n_samples=500, n_features=2, n_classes=2, random_state=1)

X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5, n_samples=400, n_features=2, n_classes=2, random_state=1)

# å°†ä¸¤ç»„æ•°æ®åˆå¹¶ä¸ºä¸€ç»„
X = np.concatenate((X1, X2))
y = np.concatenate((y1, -y2 + 1))

plt.scatter(X[:, 0], X[:, 1], marker="o", c=y)
plt.show()

# ä½¿ç”¨åŸºäºå†³ç­–æ ‘çš„Adaboostæ¥åšåˆ†ç±»
# è¿™é‡Œæˆ‘ä»¬é€‰æ‹©äº†SAMMEç®—æ³•ï¼Œæœ€å¤š200ä¸ªå¼±åˆ†ç±»å™¨ï¼Œæ­¥é•¿0.8
bdt1 = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),
                          algorithm="SAMME",
                          n_estimators=200, learning_rate=0.8)
bdt1.fit(X, y)

# æ¨¡å‹æ‹Ÿåˆå®Œæ¯•ï¼Œæˆ‘ä»¬ç”¨ç½‘æ ¼å›¾æ¥æŸ¥çœ‹æ‹Ÿåˆçš„åŒºåŸŸ
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                     np.arange(y_min, y_max, 0.02))

Z = bdt1.predict(np.c_[xx.ravel(), yy.ravel()])
# æ³¨æ„è¿™é‡Œï¼Œç›´æ¥è°ƒç”¨np.reshapeå‡½æ•°ï¼Œè€Œä¸è¦Z.reshape()
zz = np.reshape(Z, xx.shape)

cs = plt.contourf(xx, yy, zz)
plt.scatter(X[:, 0], X[:, 1], marker="o", c=y)
plt.show()

# æŸ¥çœ‹å‡†ç¡®ç‡
print(f"1st Score: {bdt1.score(X, y)}")

# ç°åœ¨æˆ‘ä»¬å°†å¼±åˆ†ç±»å™¨ä¸ªæ•°æå‡åˆ°300
bdt2 = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),
                          algorithm="SAMME",
                          n_estimators=300, learning_rate=0.8)
bdt2.fit(X, y)
print(f"2nd Score: {bdt2.score(X, y)}")
# è¿™å°è¯äº†æˆ‘ä»¬å‰é¢è®²çš„ï¼Œå¼±åˆ†ç¦»å™¨ä¸ªæ•°è¶Šå¤šï¼Œåˆ™æ‹Ÿåˆç¨‹åº¦è¶Šå¥½ï¼Œå½“ç„¶ä¹Ÿè¶Šå®¹æ˜“è¿‡æ‹Ÿåˆ

# ç°åœ¨æˆ‘ä»¬é™ä½æ­¥é•¿ï¼Œå³å‡å°‘learning rate
bdt3 = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),
                          algorithm="SAMME",
                          n_estimators=300, learning_rate=0.5)
bdt3.fit(X, y)
print(f"3rd Score: {bdt3.score(X, y)}")
# å¯è§åœ¨åŒæ ·çš„å¼±åˆ†ç±»å™¨çš„ä¸ªæ•°æƒ…å†µä¸‹ï¼Œå¦‚æœå‡å°‘æ­¥é•¿ï¼Œæ‹Ÿåˆæ•ˆæœä¼šä¸‹é™

# æœ€åï¼Œæˆ‘ä»¬å°†å¼±åˆ†ç±»å™¨ä¸ªæ•°è®¾ä¸º700ï¼Œ æ­¥é•¿ä¸º0.7
bdt4 = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),
                          algorithm="SAMME",
                          n_estimators=700, learning_rate=0.7)
bdt4.fit(X, y)
print(f"4th Score: {bdt4.score(X, y)}")
# æ­¤æ—¶çš„æ‹Ÿåˆåˆ†æ•°å’Œæˆ‘ä»¬æœ€åˆçš„300å¼±åˆ†ç±»å™¨ï¼Œ0.8æ­¥é•¿çš„æ‹Ÿåˆç¨‹åº¦ç›¸å½“ã€‚
# ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨æˆ‘ä»¬è¿™ä¸ªä¾‹å­ä¸­ï¼Œå¦‚æœæ­¥é•¿ä»0.8é™åˆ°0.7ï¼Œåˆ™å¼±åˆ†ç±»å™¨ä¸ªæ•°è¦ä»300å¢åŠ åˆ°700æ‰èƒ½è¾¾åˆ°ç±»ä¼¼çš„æ‹Ÿåˆæ•ˆæœã€‚
# å¯è§ï¼Œå¼±åˆ†ç±»å™¨ä¸ªæ•°å’Œæ­¥é•¿éœ€è¦åŒæ­¥è¿›è¡Œè°ƒèŠ‚
